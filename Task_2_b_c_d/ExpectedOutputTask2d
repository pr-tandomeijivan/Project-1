Accuracy on the permutated MNIST compared with the results on the normal MNIST.
Analysis of the results in your own words.

------------------------------------------------------------------------------------
PR_CNN

normal MNIST with best parameters (lr=0.1, epochs=5):
    test  epoch[4]: Acc@1=98.020	Loss=0.0598
another run with same settings:
    test  epoch[4]: Acc@1=98.220	Loss=0.0597


permutated MNIST with best parameters from normal MNIST (lr=0.1, epochs=5):
    first run:   test  epoch[4]: Acc@1=92.520	Loss=0.2443
    second run:  test  epoch[4]: Acc@1=96.420	Loss=0.1176
    third run:   test  epoch[4]: Acc@1=96.330	Loss=0.1196

permutated MNIST with best parameters after optimizing on the permutated dataset (lr=0.1, epochs=20):
    test  epoch[19]: Acc@1=96.700	Loss=0.1154
another run with same settings:
    test  epoch[19]: Acc@1=96.780	Loss=0.1329

------------------------------------------------------------------------------------


MLP

normal MNIST with best parameters
    test epoch[29]:    Acc@92.450  Loss=0.2734

permutated MNIST with best parameters from normal MNIST
    test  epoch[29]: Acc@1=92.310	Loss=0.2760


------------------------------------------------------------------------------------

Analysis of the results in our own words.

There is a difference for the CNN:

Convolutions are "looking" for structural patterns (like a - or |)  in the data. In the non-permutated
dataset we imagine this to be easier than in the permutated dataset, as the permutation destroys the
structures and pixels that are normally close together are not close anymore. Obviously, it works quite
well anyway, so somehow we think there must be some structures that can be found also in the permutated
images, if it is not all due to the final fc layer that might have a huge impact.

For the FC the difference is very small, if existing at all:

It is basically just a fc layer, thus the structure of the pixels does not play a role.